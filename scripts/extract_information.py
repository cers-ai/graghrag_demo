#!/usr/bin/env python3
"""
ä¿¡æ¯æŠ½å–å‘½ä»¤è¡Œå·¥å…·
"""

import sys
import json
import argparse
from pathlib import Path
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from backend.app.services.information_extractor import InformationExtractor
from backend.app.services.schema_manager import SchemaManager
from backend.app.services.ollama_client import OllamaClient
from backend.app.services.document_parser import DocumentParser


def extract_from_text(args):
    """ä»æ–‡æœ¬æŠ½å–ä¿¡æ¯"""
    logger.info("=" * 50)
    logger.info("æ–‡æœ¬ä¿¡æ¯æŠ½å–")
    logger.info("=" * 50)

    # åˆå§‹åŒ–ç»„ä»¶
    schema_manager = SchemaManager(args.schema_file)
    ollama_client = OllamaClient(
        model=args.model,
        prompts_dir=args.prompts_dir
    )
    extractor = InformationExtractor(
        schema_manager=schema_manager,
        ollama_client=ollama_client,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap
    )

    try:
        # åŠ è½½Schema
        schema_manager.load_schema()
        logger.info("âœ… SchemaåŠ è½½æˆåŠŸ")

        # æµ‹è¯•Ollamaè¿æ¥
        if not ollama_client.test_connection():
            logger.error("âŒ Ollamaè¿æ¥å¤±è´¥")
            return False

        # è·å–è¾“å…¥æ–‡æœ¬
        if args.text:
            text = args.text
        elif args.file:
            with open(args.file, 'r', encoding='utf-8') as f:
                text = f.read()
        else:
            logger.error("è¯·æä¾›æ–‡æœ¬å†…å®¹æˆ–æ–‡ä»¶è·¯å¾„")
            return False

        logger.info(f"ğŸ“ è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")

        # æ‰§è¡ŒæŠ½å–
        logger.info("ğŸ” å¼€å§‹ä¿¡æ¯æŠ½å–...")
        result = extractor.extract_from_text(text)

        if result.success:
            logger.info("âœ… ä¿¡æ¯æŠ½å–æˆåŠŸï¼")

            # æ˜¾ç¤ºç»“æœ
            logger.info(f"\nğŸ“Š æŠ½å–ç»“æœ:")
            logger.info(f"  å®ä½“æ•°é‡: {len(result.entities)}")
            logger.info(f"  å…³ç³»æ•°é‡: {len(result.relations)}")

            if args.verbose:
                logger.info(f"\nğŸ“‹ å®ä½“åˆ—è¡¨:")
                for i, entity in enumerate(result.entities, 1):
                    logger.info(f"  {i:2d}. {entity.type}: {entity.name}")
                    if entity.properties:
                        for key, value in entity.properties.items():
                            logger.info(f"      {key}: {value}")
                    logger.info(f"      ç½®ä¿¡åº¦: {entity.confidence:.2f}")

                logger.info(f"\nğŸ“‹ å…³ç³»åˆ—è¡¨:")
                for i, relation in enumerate(result.relations, 1):
                    logger.info(f"  {i:2d}. {relation.source} --[{relation.type}]--> {relation.target}")
                    if relation.properties:
                        for key, value in relation.properties.items():
                            logger.info(f"      {key}: {value}")
                    logger.info(f"      ç½®ä¿¡åº¦: {relation.confidence:.2f}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = extractor.get_extraction_stats(result)
            logger.info(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"  å®ä½“ç±»å‹åˆ†å¸ƒ: {stats['entity_types']}")
            logger.info(f"  å…³ç³»ç±»å‹åˆ†å¸ƒ: {stats['relation_types']}")
            if result.metadata:
                logger.info(f"  å¤„ç†æ—¶é—´: {result.metadata.get('processing_time', 0):.2f}ç§’")
                logger.info(f"  æ–‡æœ¬å—æ•°: {result.metadata.get('chunks_count', 1)}")

            # éªŒè¯ç»“æœ
            if args.validate:
                logger.info(f"\nğŸ” éªŒè¯æŠ½å–ç»“æœ...")
                errors = extractor.validate_extraction_result(result)
                if errors:
                    logger.warning(f"å‘ç° {len(errors)} ä¸ªéªŒè¯é”™è¯¯:")
                    for error in errors:
                        logger.warning(f"  - {error}")
                else:
                    logger.info("âœ… éªŒè¯é€šè¿‡ï¼Œæ— é”™è¯¯")

            # ä¿å­˜ç»“æœ
            if args.output:
                output_data = {
                    'entities': [
                        {
                            'type': e.type,
                            'name': e.name,
                            'properties': e.properties,
                            'confidence': e.confidence
                        } for e in result.entities
                    ],
                    'relations': [
                        {
                            'type': r.type,
                            'source': r.source,
                            'target': r.target,
                            'properties': r.properties,
                            'confidence': r.confidence
                        } for r in result.relations
                    ],
                    'metadata': {
                        'source_text_length': len(result.source_text),
                        'extraction_time': result.extraction_time.isoformat(),
                        'stats': stats
                    }
                }

                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, ensure_ascii=False, indent=2)

                logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

            return True
        else:
            logger.error(f"âŒ ä¿¡æ¯æŠ½å–å¤±è´¥: {result.error_message}")
            return False

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        return False


def extract_from_document(args):
    """ä»æ–‡æ¡£æŠ½å–ä¿¡æ¯"""
    logger.info("=" * 50)
    logger.info("æ–‡æ¡£ä¿¡æ¯æŠ½å–")
    logger.info("=" * 50)

    # åˆå§‹åŒ–ç»„ä»¶
    schema_manager = SchemaManager(args.schema_file)
    ollama_client = OllamaClient(
        model=args.model,
        prompts_dir=args.prompts_dir
    )
    extractor = InformationExtractor(
        schema_manager=schema_manager,
        ollama_client=ollama_client,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap
    )
    document_parser = DocumentParser()

    try:
        # åŠ è½½Schema
        schema_manager.load_schema()
        logger.info("âœ… SchemaåŠ è½½æˆåŠŸ")

        # è§£ææ–‡æ¡£
        logger.info(f"ğŸ“„ è§£ææ–‡æ¡£: {args.document}")
        parsed_doc = document_parser.parse_document(args.document)

        if not parsed_doc.success:
            logger.error(f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {parsed_doc.error_message}")
            return False

        logger.info(f"âœ… æ–‡æ¡£è§£ææˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(parsed_doc.content)} å­—ç¬¦")

        # æ‰§è¡ŒæŠ½å–
        logger.info("ğŸ” å¼€å§‹ä¿¡æ¯æŠ½å–...")
        result = extractor.extract_from_text(parsed_doc.content)

        if result.success:
            logger.info("âœ… ä¿¡æ¯æŠ½å–æˆåŠŸï¼")

            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            logger.info(f"\nğŸ“Š æŠ½å–ç»“æœ:")
            logger.info(f"  æ–‡æ¡£: {parsed_doc.title}")
            logger.info(f"  å®ä½“æ•°é‡: {len(result.entities)}")
            logger.info(f"  å…³ç³»æ•°é‡: {len(result.relations)}")

            # ä¿å­˜ç»“æœ
            if args.output:
                output_data = {
                    'document_info': {
                        'file_path': parsed_doc.file_path,
                        'title': parsed_doc.title,
                        'file_type': parsed_doc.file_type,
                        'metadata': parsed_doc.metadata
                    },
                    'extraction_result': {
                        'entities': [
                            {
                                'type': e.type,
                                'name': e.name,
                                'properties': e.properties,
                                'confidence': e.confidence
                            } for e in result.entities
                        ],
                        'relations': [
                            {
                                'type': r.type,
                                'source': r.source,
                                'target': r.target,
                                'properties': r.properties,
                                'confidence': r.confidence
                            } for r in result.relations
                        ],
                        'extraction_time': result.extraction_time.isoformat(),
                        'stats': extractor.get_extraction_stats(result)
                    }
                }

                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, ensure_ascii=False, indent=2)

                logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

            return True
        else:
            logger.error(f"âŒ ä¿¡æ¯æŠ½å–å¤±è´¥: {result.error_message}")
            return False

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        return False


def test_extraction(args):
    """æµ‹è¯•ä¿¡æ¯æŠ½å–åŠŸèƒ½"""
    logger.info("=" * 50)
    logger.info("ä¿¡æ¯æŠ½å–åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 50)

    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    å¼ ä¸‰æ˜¯ABCç§‘æŠ€å…¬å¸çš„é¡¹ç›®ç»ç†ï¼Œè´Ÿè´£ç®¡ç†çŸ¥è¯†å›¾è°±é¡¹ç›®ã€‚
    æå››æ˜¯è¯¥å…¬å¸çš„é«˜çº§å¼€å‘å·¥ç¨‹å¸ˆï¼Œå‚ä¸é¡¹ç›®çš„æŠ€æœ¯å¼€å‘å·¥ä½œã€‚
    ç‹äº”æ˜¯äº§å“ç»ç†ï¼Œè´Ÿè´£éœ€æ±‚åˆ†æå’Œäº§å“è®¾è®¡ã€‚
    è¿™ä¸ªé¡¹ç›®å±äºæŠ€æœ¯ç ”å‘éƒ¨é—¨ï¼Œé¢„è®¡åœ¨2024å¹´6æœˆå®Œæˆã€‚
    """

    logger.info(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text.strip()}")

    # ä½¿ç”¨æµ‹è¯•æ–‡æœ¬è¿›è¡ŒæŠ½å–
    args.text = test_text
    args.file = None

    return extract_from_text(args)


def main():
    """ä¸»å‡½æ•°"""
    arg_parser = argparse.ArgumentParser(description="ä¿¡æ¯æŠ½å–å·¥å…·")

    # å…¨å±€å‚æ•°
    arg_parser.add_argument("-s", "--schema-file", default="config/schema.yaml", help="Schemaæ–‡ä»¶è·¯å¾„")
    arg_parser.add_argument("-m", "--model", default="qwen3:4b", help="ä½¿ç”¨çš„æ¨¡å‹")
    arg_parser.add_argument("-p", "--prompts-dir", default="config/prompts", help="æç¤ºè¯ç›®å½•")
    arg_parser.add_argument("--chunk-size", type=int, default=2000, help="æ–‡æœ¬åˆ†å—å¤§å°")
    arg_parser.add_argument("--chunk-overlap", type=int, default=200, help="åˆ†å—é‡å å¤§å°")
    arg_parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")
    arg_parser.add_argument("--validate", action="store_true", help="éªŒè¯æŠ½å–ç»“æœ")
    arg_parser.add_argument("-o", "--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    subparsers = arg_parser.add_subparsers(dest="command", help="å‘½ä»¤")

    # æ–‡æœ¬æŠ½å–
    text_parser = subparsers.add_parser("text", help="ä»æ–‡æœ¬æŠ½å–ä¿¡æ¯")
    text_group = text_parser.add_mutually_exclusive_group(required=True)
    text_group.add_argument("-t", "--text", help="è¾“å…¥æ–‡æœ¬")
    text_group.add_argument("-f", "--file", help="æ–‡æœ¬æ–‡ä»¶è·¯å¾„")

    # æ–‡æ¡£æŠ½å–
    doc_parser = subparsers.add_parser("document", help="ä»æ–‡æ¡£æŠ½å–ä¿¡æ¯")
    doc_parser.add_argument("document", help="æ–‡æ¡£æ–‡ä»¶è·¯å¾„")

    # æµ‹è¯•æ¨¡å¼
    subparsers.add_parser("test", help="æµ‹è¯•ä¿¡æ¯æŠ½å–åŠŸèƒ½")

    args = arg_parser.parse_args()

    if not args.command:
        arg_parser.print_help()
        return

    try:
        if args.command == "text":
            success = extract_from_text(args)
        elif args.command == "document":
            success = extract_from_document(args)
        elif args.command == "test":
            success = test_extraction(args)
        else:
            logger.error(f"æœªçŸ¥å‘½ä»¤: {args.command}")
            success = False

        sys.exit(0 if success else 1)

    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
